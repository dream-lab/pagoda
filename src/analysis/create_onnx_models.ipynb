{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate for Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # Example input with batch size 1\n",
    "\n",
    "export(model, dummy_input, \"../onnx_models/resnet50.onnx\", opset_version=12,\n",
    "       input_names=['input'], output_names=['output'],\n",
    "       dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 10:11:42.989807: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-04 10:11:44.054617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738644104.427807 4086798 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738644104.530204 4086798 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 10:11:45.336858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/saisamarth/miniconda3/lib/python3.11/site-packages/torch/onnx/utils.py:1531: OnnxExporterWarning: Exporting to ONNX opset version 19 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 17. To use a newer opset version, consider 'torch.onnx.dynamo_export()'. Note that dynamo_export() is in preview. Please report errors with dynamo_export() as Github issues to https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported BERT model with static shapes to ONNX!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from torch.onnx import export\n",
    "import torch\n",
    "\n",
    "# Load the pretrained BERT model and tokenizer\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()\n",
    "\n",
    "# Define static inputs\n",
    "sequence_length = 128\n",
    "batch_size = 1\n",
    "input_ids = torch.randint(0, 30522, (batch_size, sequence_length), dtype=torch.int64)\n",
    "attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.int64)\n",
    "\n",
    "# Export the model to ONNX with fixed dimensions\n",
    "export(\n",
    "    model,\n",
    "    args=(input_ids, attention_mask),\n",
    "    f=\"../onnx_models/bert-base.onnx\",\n",
    "    opset_version=19,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"last_hidden_state\", \"pooler_output\"]\n",
    ")\n",
    "\n",
    "print(\"Exported BERT model with static shapes to ONNX!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate for LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.onnx import export\n",
    "\n",
    "# Define a simple LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, \"cannot tie, check dims\"\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1 / math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(\n",
    "                self.embedding_dim, self.hidden_dim\n",
    "            ).uniform_(-init_range_other, init_range_other)\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(\n",
    "                self.hidden_dim, self.hidden_dim\n",
    "            ).uniform_(-init_range_other, init_range_other)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 29423\n",
    "embedding_dim = 256  # Match hidden_dim for tied weights\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "dropout_rate = 0.2\n",
    "tie_weights = True  # Tied weights now valid\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "seq_length = 32\n",
    "\n",
    "# Initialize the model\n",
    "lstm_model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights)\n",
    "lstm_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Example input for the LSTM model\n",
    "inputs = torch.randint(0, vocab_size, (batch_size, seq_length), dtype=torch.long)\n",
    "hidden_ = (torch.zeros(num_layers, batch_size, hidden_dim),\n",
    "           torch.zeros(num_layers, batch_size, hidden_dim))\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_file_path = \"../onnx_models/lstm.onnx\"\n",
    "export(\n",
    "    lstm_model,\n",
    "    args=(inputs, hidden_),  # Model input with optional hidden state\n",
    "    f=onnx_file_path,\n",
    "    opset_version=19,  # Compatible ONNX opset for LSTM\n",
    "    input_names=[\"input\", \"hidden\"],  # Input tensor name\n",
    "    output_names=[\"output\", \"hidden_out\"],  # Output tensor names\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\", 1: \"sequence_length\"},  # Dynamic axes\n",
    "        \"hidden\": {1: \"batch_size\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"LSTM model exported to {onnx_file_path}!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Specify model weights and export path\n",
    "model_weights = 'yolov8n.pt'  # Path to YOLOv8-n weights\n",
    "export_path = '../onnx_models/'  # Desired export path\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(model_weights)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "model.export(format='onnx', imgsz=640, dynamic=False)\n",
    "!mv yolov8n.onnx export_path\n",
    "\n",
    "print(f\"Model exported to ONNX format at: {export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate for MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # Example input with batch size 1\n",
    "\n",
    "export(model, dummy_input, \"../onnx_models/mobnet.onnx\", opset_version=19,\n",
    "       input_names=['input'], output_names=['output'],\n",
    "       dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval()\n",
    "layer = model.features[16][0]\n",
    "\n",
    "dummy_input = torch.randn(1, 160, 7, 7)  # Example input with batch size 1\n",
    "\n",
    "export(layer, dummy_input, \"../onnx_models/mobnet_pw_conv.onnx\", opset_version=19,\n",
    "       input_names=['input'], output_names=['output'],\n",
    "       dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saisamarth/miniconda3/lib/python3.11/site-packages/torch/onnx/utils.py:1531: OnnxExporterWarning: Exporting to ONNX opset version 19 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 17. To use a newer opset version, consider 'torch.onnx.dynamo_export()'. Note that dynamo_export() is in preview. Please report errors with dynamo_export() as Github issues to https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.onnx import export\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "model.eval()\n",
    "layer = model.features[4]\n",
    "conv1 = layer.block[0][0]\n",
    "\n",
    "dummy_input = torch.randn(1, 24, 56, 56)  # Example input with batch size 1\n",
    "\n",
    "export(conv1, dummy_input, \"../onnx_models/mobnet_bottleneck_conv1.onnx\", opset_version=19,\n",
    "       input_names=['input'], output_names=['output'],\n",
    "       dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "conv1 = torch.nn.Conv2d(160, 960, (1, 1), (1, 1), bias=False).eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 160, 56, 56)  # Example input with batch size 1\n",
    "\n",
    "export(conv1, dummy_input, \"../onnx_models/mobnet_bottleneck_conv2.onnx\", opset_version=12,\n",
    "       input_names=['input'], output_names=['output'],\n",
    "       dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate for FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.onnx import export\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear model\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight.T) + self.bias\n",
    "\n",
    "# Parameters\n",
    "input_dim = 64\n",
    "output_dim = 128\n",
    "\n",
    "# Initialize the model\n",
    "linear_model = SimpleLinearModel(input_dim, output_dim)\n",
    "linear_model.eval()\n",
    "\n",
    "# Example input for the linear model\n",
    "example_input = torch.randn(1, input_dim)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_file_path = \"../onnx_models/simple_linear_64_128.onnx\"\n",
    "export(\n",
    "    linear_model,\n",
    "    args=(example_input),\n",
    "    f=onnx_file_path,\n",
    "    opset_version=12,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Linear model exported to {onnx_file_path}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
